![[imbalanced-learn.org-over_sampling.pdf]]

---

#### Page 1

> 2.1.1. Naive random over-sampling (p. 1) 

> The most naive strategy is to generate new samples by randomly sampling with replacement the current available samples. (p. 1) 

sample with replacement = Ctrl c ctrl v (p. 1) 

> In the �gure below, we compare the decision functions of a classi�er trained using the over-sampled data set and the original data set. (p. 1) 

```pdf
{"url": "/Papers/pdfs/Archived/imbalanced-learn.org-over_sampling.pdf", "page": 1,
"rect": [0,0,0,0]
}
```

---
#### Page 2

> If repeating samples is an issue, the parameter shrinkage allows to create a smoothed bootstrap. However, the original data needs to be numerical. The shrinkage parameter controls the dispersion of the new generated (p. 2) 

> a Random Over- Sampling Examples (p. 2) 

> generating (p. 2) 

```pdf
{"url": "/Papers/pdfs/Archived/imbalanced-learn.org-over_sampling.pdf", "page": 2,
"rect": [0,0,0,0]
}
```

> Apart from the random sampling with replacement, there are two popular methods to over-sample minority classes: (i) the Synthetic Minority Oversampling Technique (SMOTE) [[CBHK02](https://imbalanced-learn.org/stable/zzz_references.html#id12)] and (ii) the Adaptive Synthetic (ADASYN) [[HBGL08](https://imbalanced-learn.org/stable/zzz_references.html#id11)] sampling method. (p. 2) 

---
#### Page 3

> While the[ RandomOverSampler ](https://imbalanced-learn.org/stable/references/generated/imblearn.over_sampling.RandomOverSampler.html#imblearn.over_sampling.RandomOverSampler)is over-sampling by duplicating some of the original samples of the minority class,[ SMOTE ](https://imbalanced-learn.org/stable/references/generated/imblearn.over_sampling.SMOTE.html#imblearn.over_sampling.SMOTE)and[ ADASYN ](https://imbalanced-learn.org/stable/references/generated/imblearn.over_sampling.ADASYN.html#imblearn.over_sampling.ADASYN)generate new samples in by interpolation. However, the samples used to interpolate/generate new synthetic samples differ. In fact, ADASYN focuses on generating samples next to the original samples which are wrongly classi�ed using a k-Nearest Neighbors classi�er while the basic implementation of SMOTE will not make any distinction between easy and hard samples to be classi�ed using the nearest neighbors rule. Therefore, the decision function found during training will be different among the algorithms. (p. 3) 

---
#### Page 4

> SMOTE might connect inliers and outliers while ADASYN might focus solely on outliers which, in both cases, might lead to a sub-optimal decision function. In this regard, SMOTE offers three additional options to generate samples. Those methods focus on samples near of the border of the optimal decision function and will generate samples in the opposite direction of the nearest neighbors class. (p. 4) 

---
#### Page 7

> Both[ SMOTE ](https://imbalanced-learn.org/stable/references/generated/imblearn.over_sampling.SMOTE.html#imblearn.over_sampling.SMOTE)and[ ADASYN ](https://imbalanced-learn.org/stable/references/generated/imblearn.over_sampling.ADASYN.html#imblearn.over_sampling.ADASYN)use the same algorithm to generate new (p. 7) 

> 2.2.1. Sample generation (p. 7) 

```pdf
{"url": "/Papers/pdfs/Archived/imbalanced-learn.org-over_sampling.pdf", "page": 7,
"rect": [0,0,0,0]
}
```

> SMOTE-NC slightly change the way a new sample is generated by performing something speci�c for the categorical features. In fact, the categories of a new generated sample are decided by picking the most frequent category of the nearest neighbors present during the generation. (p. 7) 

> Warning  Be aware that SMOTE-NC is not designed to work with only categorical data. (p. 7) 

> The regular SMOTE algorithm — cf. to the SMOTE object — does not impose any rule and will randomly pick-up all possible available. (p. 7) 

---
#### Page 8

> The borderline[ SMOTE ](https://imbalanced-learn.org/stable/references/generated/imblearn.over_sampling.SMOTE.html#imblearn.over_sampling.SMOTE)— cf. to the[ BorderlineSMOTE ](https://imbalanced-learn.org/stable/references/generated/imblearn.over_sampling.BorderlineSMOTE.html#imblearn.over_sampling.BorderlineSMOTE)with the parameters kind='borderline-1' and kind='borderline-2' — will classify each sample to be (i) noise (i.e. all nearest-neighbors are from a different class than the one of ), (ii) in danger (i.e. at least half of the nearest neighbors are from the same class than , or (iii) safe (i.e. all nearest neighbors are from the same class than ). Borderline-1 and Borderline-2 SMOTE will use the samples in danger to generate new samples. In Borderline-1 SMOTE, will belong to the same class than the one of the sample . On the contrary, Borderline-2 SMOTE will consider which can be from any class. (p. 8) 

> SVM SMOTE — cf. to[ SVMSMOTE ](https://imbalanced-learn.org/stable/references/generated/imblearn.over_sampling.SVMSMOTE.html#imblearn.over_sampling.SVMSMOTE)— uses an SVM classi�er to �nd support vectors and generate samples considering them. Note that the C parameter of the SVM classi�er allows to select more or less support vectors. For both borderline and SVM SMOTE, a neighborhood is de�ned using the parameter m_neighbors to decide if a sample is in danger, safe, or noise. (p. 8) 

> KMeans SMOTE — cf. to[ KMeansSMOTE ](https://imbalanced-learn.org/stable/references/generated/imblearn.over_sampling.KMeansSMOTE.html#imblearn.over_sampling.KMeansSMOTE)— uses a KMeans clustering method before to apply[ SMOTE.](https://imbalanced-learn.org/stable/references/generated/imblearn.over_sampling.SMOTE.html#imblearn.over_sampling.SMOTE) The clustering will group samples together and generate new samples depending of the cluster density. (p. 8) 

> ADASYN works similarly to the regular SMOTE. However, the number of samples generated for each is proportional to the number of samples which are not from the same class than in a given neighborhood. Therefore, more samples will be generated in the area that the nearest neighbor rule is not (p. 8) 

---
